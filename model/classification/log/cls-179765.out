=============================================
[INFO] Job Information
---------------------------------------------
[INFO] Job Name       : cls_framework
[INFO] Job ID         : 179765
[INFO] Submit Dir     : /home/cseomoon/appl/af_analysis-0.1.4/model/classification/scripts
[INFO] Partition      : g4090_short
[INFO] Node List      : gpu10
[INFO] Nodes          : 1
[INFO] CPUs per Task  : 32
[INFO] Memory (MB)    : 
[INFO] User           : cseomoon
[INFO] Work Dir       : /home/cseomoon/appl/af_analysis-0.1.4/model/classification/scripts
[INFO] Tasks per Node : 1
[INFO] Dependency     : 
[INFO] GPU(s)         : 
=============================================

[INFO] Job started at : Wed Apr 23 18:52:58 KST 2025
[INFO] Job allocated on node(s): gpu10
[INFO] Job running on: gpu10
Using 32 CPU cores.
Results will be saved to: classification_run_1
Results saved to classification_run_1/run_arguments.json

--- Loading Data ---
Original data shape: (3650, 81)
Class distribution (DockQ >= 0.23): 0 (Negative) = 2529, 1 (Positive) = 1121
Columns to drop for features (X): ['chain_pair_pae_min', 'chain_ptm', 'rRMS', 'iRMS', 'Fnonnat', 'LRMS', 'model_path', 'Fnat', 'DockQ', 'seed', 'data_file', 'chain_iptm', 'chain_pair_iptm', 'native_path', 'pdb', 'query', 'sample', 'format']
Processed Features (X) shape: (3650, 63)
Processed Target (y) shape: (3650,)
Processed Query IDs shape: (3650,)

--- Starting Model Training ---

--- Running Nested CV for: Random Forest (rf) ---
Output directory: classification_run_1/rf
Using GroupKFold for outer CV with 2 folds based on query IDs.

-- Processing Outer Fold 1/2 --
Train set size: (1800, 63), Test set size: (1850, 63)
Test set indices range from 0 to 3499
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None, 'class_weight': 'balanced'}
Best Inner CV ROC AUC score: 0.9178
Results saved to classification_run_1/rf/fold_1/best_params.json
Predicting on outer test set...
Calculating performance metrics...
Results saved to classification_run_1/rf/fold_1/metrics.json
Predictions saved to classification_run_1/rf/fold_1/predictions.csv
Calculating and saving SHAP values...
Calculating SHAP values using TreeExplainer for RandomForest...
SHAP explainer returned a single array. Returning as is.
Error saving SHAP values: Must pass 2-d input. shape=(1850, 63, 2)

-- Processing Outer Fold 2/2 --
Train set size: (1850, 63), Test set size: (1800, 63)
Test set indices range from 50 to 3649
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 40, 'class_weight': None}
Best Inner CV ROC AUC score: 0.9571
Results saved to classification_run_1/rf/fold_2/best_params.json
Predicting on outer test set...
Calculating performance metrics...
Results saved to classification_run_1/rf/fold_2/metrics.json
Predictions saved to classification_run_1/rf/fold_2/predictions.csv
Calculating and saving SHAP values...
Calculating SHAP values using TreeExplainer for RandomForest...
SHAP explainer returned a single array. Returning as is.
Error saving SHAP values: Must pass 2-d input. shape=(1800, 63, 2)

--- Aggregating results for: Random Forest (rf) ---
Average Metrics across folds:
{
    "accuracy_mean": 0.8840315315315315,
    "accuracy_std": 0.02569819819819824,
    "precision_mean": 0.8240430783796577,
    "precision_std": 0.041785013863528786,
    "recall_mean": 0.8109396415899164,
    "recall_std": 0.12716480715283024,
    "f1_mean": 0.8086147797936208,
    "f1_std": 0.04450747174551728,
    "roc_auc_mean": 0.9403314114488173,
    "roc_auc_std": 0.01840899899228443,
    "pr_auc_mean": 0.9011987908553369,
    "pr_auc_std": 0.020588361957865564,
    "balanced_accuracy_mean": 0.8667730646661267,
    "balanced_accuracy_std": 0.05164150029039294,
    "mcc_mean": 0.7349184589200014,
    "mcc_std": 0.060597204747929956,
    "best_inner_cv_roc_auc_mean": 0.9374143015122154,
    "best_inner_cv_roc_auc_std": 0.01965022995256782
}
Results saved to classification_run_1/rf/metrics_summary.json

Running global SHAP analysis...
No SHAP value files ('shap_values_fold_*.csv') found in classification_run_1/rf. Cannot perform global analysis.
--- Nested CV completed for: Random Forest (rf) ---

--- Running Nested CV for: XGBoost (xgb) ---
Output directory: classification_run_1/xgb
Using GroupKFold for outer CV with 2 folds based on query IDs.

-- Processing Outer Fold 1/2 --
Train set size: (1800, 63), Test set size: (1850, 63)
Test set indices range from 0 to 3499
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'subsample': 0.8, 'scale_pos_weight': 1, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.6}
Best Inner CV ROC AUC score: 0.9095
Results saved to classification_run_1/xgb/fold_1/best_params.json
Predicting on outer test set...
Calculating performance metrics...
Results saved to classification_run_1/xgb/fold_1/metrics.json
Predictions saved to classification_run_1/xgb/fold_1/predictions.csv
Calculating and saving SHAP values...
Calculating SHAP values using TreeExplainer for XGBoost...
Returning SHAP values (expected for positive class log-odds).
SHAP values saved to classification_run_1/xgb/fold_1/shap_values_fold_1.csv
Test data corresponding to SHAP values saved to classification_run_1/xgb/fold_1/test_data_fold_1.csv

-- Processing Outer Fold 2/2 --
Train set size: (1850, 63), Test set size: (1800, 63)
Test set indices range from 50 to 3649
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'subsample': 0.8, 'scale_pos_weight': 5, 'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 0.5, 'colsample_bytree': 0.6}
Best Inner CV ROC AUC score: 0.9461
Results saved to classification_run_1/xgb/fold_2/best_params.json
Predicting on outer test set...
Calculating performance metrics...
Results saved to classification_run_1/xgb/fold_2/metrics.json
Predictions saved to classification_run_1/xgb/fold_2/predictions.csv
Calculating and saving SHAP values...
Calculating SHAP values using TreeExplainer for XGBoost...
Returning SHAP values (expected for positive class log-odds).
SHAP values saved to classification_run_1/xgb/fold_2/shap_values_fold_2.csv
Test data corresponding to SHAP values saved to classification_run_1/xgb/fold_2/test_data_fold_2.csv

--- Aggregating results for: XGBoost (xgb) ---
Average Metrics across folds:
{
    "accuracy_mean": 0.8788513513513514,
    "accuracy_std": 0.023851351351351354,
    "precision_mean": 0.809900490220029,
    "precision_std": 0.04370775720265141,
    "recall_mean": 0.8117674561594528,
    "recall_std": 0.12633699258329384,
    "f1_mean": 0.801904419690981,
    "f1_std": 0.04157384117858426,
    "roc_auc_mean": 0.9369083621359502,
    "roc_auc_std": 0.019514012316463658,
    "pr_auc_mean": 0.8972197401928295,
    "pr_auc_std": 0.021717914930329818,
    "balanced_accuracy_mean": 0.8632856517211718,
    "balanced_accuracy_std": 0.050252694180584034,
    "mcc_mean": 0.7242875090566587,
    "mcc_std": 0.05798664946696469,
    "best_inner_cv_roc_auc_mean": 0.9278163090990069,
    "best_inner_cv_roc_auc_std": 0.01831698672274218
}
Results saved to classification_run_1/xgb/metrics_summary.json

Running global SHAP analysis...
No SHAP value files ('shap_values_fold_*.csv') found in classification_run_1/xgb. Cannot perform global analysis.
--- Nested CV completed for: XGBoost (xgb) ---

--- Running Nested CV for: LightGBM (lgb) ---
Output directory: classification_run_1/lgb
Using GroupKFold for outer CV with 2 folds based on query IDs.

-- Processing Outer Fold 1/2 --
Train set size: (1800, 63), Test set size: (1850, 63)
Test set indices range from 0 to 3499
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'subsample': 0.8, 'reg_lambda': 0, 'reg_alpha': 0.01, 'num_leaves': 60, 'n_estimators': 200, 'max_depth': -1, 'learning_rate': 0.05, 'colsample_bytree': 0.8, 'class_weight': None}
Best Inner CV ROC AUC score: 0.9130
Results saved to classification_run_1/lgb/fold_1/best_params.json
Predicting on outer test set...
Calculating performance metrics...
Results saved to classification_run_1/lgb/fold_1/metrics.json
Predictions saved to classification_run_1/lgb/fold_1/predictions.csv
Calculating and saving SHAP values...
Calculating SHAP values using TreeExplainer for LightGBM...
SHAP explainer returned a single array. Returning as is.
SHAP values saved to classification_run_1/lgb/fold_1/shap_values_fold_1.csv
Test data corresponding to SHAP values saved to classification_run_1/lgb/fold_1/test_data_fold_1.csv

-- Processing Outer Fold 2/2 --
Train set size: (1850, 63), Test set size: (1800, 63)
Test set indices range from 50 to 3649
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'subsample': 0.8, 'reg_lambda': 5, 'reg_alpha': 0.01, 'num_leaves': 50, 'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.6, 'class_weight': None}
Best Inner CV ROC AUC score: 0.9586
Results saved to classification_run_1/lgb/fold_2/best_params.json
Predicting on outer test set...
Calculating performance metrics...
Results saved to classification_run_1/lgb/fold_2/metrics.json
Predictions saved to classification_run_1/lgb/fold_2/predictions.csv
Calculating and saving SHAP values...
Calculating SHAP values using TreeExplainer for LightGBM...
SHAP explainer returned a single array. Returning as is.
SHAP values saved to classification_run_1/lgb/fold_2/shap_values_fold_2.csv
Test data corresponding to SHAP values saved to classification_run_1/lgb/fold_2/test_data_fold_2.csv

--- Aggregating results for: LightGBM (lgb) ---
Average Metrics across folds:
{
    "accuracy_mean": 0.8777702702702703,
    "accuracy_std": 0.02277027027027029,
    "precision_mean": 0.81357603566074,
    "precision_std": 0.05054286030528976,
    "recall_mean": 0.8040385181959087,
    "recall_std": 0.1301974585932596,
    "f1_mean": 0.7986046511627907,
    "f1_std": 0.0413953488372093,
    "roc_auc_mean": 0.9394073034208088,
    "roc_auc_std": 0.03135424576726181,
    "pr_auc_mean": 0.9069615079035886,
    "pr_auc_std": 0.04157845952264738,
    "balanced_accuracy_mean": 0.8605092996682976,
    "balanced_accuracy_std": 0.05034462270978246,
    "mcc_mean": 0.7217968880818033,
    "mcc_std": 0.05539925246860056,
    "best_inner_cv_roc_auc_mean": 0.9357888735214903,
    "best_inner_cv_roc_auc_std": 0.022815578904382794
}
Results saved to classification_run_1/lgb/metrics_summary.json

Running global SHAP analysis...
No SHAP value files ('shap_values_fold_*.csv') found in classification_run_1/lgb. Cannot perform global analysis.
--- Nested CV completed for: LightGBM (lgb) ---

--- Running Nested CV for: Logistic Regression (logistic) ---
Output directory: classification_run_1/logistic
Using GroupKFold for outer CV with 2 folds based on query IDs.

-- Processing Outer Fold 1/2 --
Train set size: (1800, 63), Test set size: (1850, 63)
Test set indices range from 0 to 3499
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Best Params found: {'model__solver': 'saga', 'model__penalty': 'elasticnet', 'model__l1_ratio': 0.2, 'model__C': 0.000774263682681127}
Best Inner CV ROC AUC score: 0.9066
Results saved to classification_run_1/logistic/fold_1/best_params.json
Predicting on outer test set...
Error during prediction or evaluation in fold 1: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values
Traceback (most recent call last):
  File "/home/cseomoon/appl/af_analysis-0.1.4/model/classification/train_models.py", line 136, in run_nested_cv
    y_prob = best_estimator.predict_proba(X_test)[:, 1] # Probability of class 1
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/pipeline.py", line 904, in predict_proba
    return self.steps[-1][1].predict_proba(Xt, **params)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py", line 1428, in predict_proba
    return super()._predict_proba_lr(X)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_base.py", line 389, in _predict_proba_lr
    prob = self.decision_function(X)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_base.py", line 351, in decision_function
    X = validate_data(self, X, accept_sparse="csr", reset=False)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 2944, in validate_data
    out = check_array(X, input_name="X", **check_params)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1107, in check_array
    _assert_all_finite(
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 120, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 169, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


-- Processing Outer Fold 2/2 --
Train set size: (1850, 63), Test set size: (1800, 63)
Test set indices range from 50 to 3649
Using GroupKFold for inner CV with 2 folds.
Starting hyperparameter tuning (RandomizedSearchCV)...
Fitting 2 folds for each of 50 candidates, totalling 100 fits
Error during RandomizedSearchCV in fold 2: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values
Traceback (most recent call last):
  File "/home/cseomoon/appl/af_analysis-0.1.4/model/classification/train_models.py", line 115, in run_nested_cv
    search.fit(X_train, y_train) # Fit on the outer training data
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/model_selection/_search.py", line 1062, in fit
    self.best_estimator_.fit(X, y, **routed_params.estimator.fit)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/pipeline.py", line 662, in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py", line 1222, in fit
    X, y = validate_data(
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 2961, in validate_data
    X, y = check_X_y(X, y, **check_params)
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1370, in check_X_y
    X = check_array(
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 1107, in check_array
    _assert_all_finite(
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 120, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/utils/validation.py", line 169, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input X contains NaN.
LogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values


Error: No metrics were collected for model logistic. Check logs for errors in folds.

--- Overall Training Summary ---
Model comparison summary saved to: classification_run_1/model_comparison_summary.csv
  model_name  accuracy_mean  ...  best_inner_cv_roc_auc_std                 error
0         rf       0.884032  ...                   0.019650                   NaN
1        xgb       0.878851  ...                   0.018317                   NaN
2        lgb       0.877770  ...                   0.022816                   NaN
3   logistic            NaN  ...                        NaN  No metrics collected

[4 rows x 20 columns]

--- Framework Execution Finished ---
작업 완료
[INFO] Job completed at: Wed Apr 23 19:01:23 KST 2025
