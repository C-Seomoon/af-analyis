2025-05-22 13:34:59,282 [INFO] Successfully imported model classes using relative paths.
2025-05-22 13:34:59,687 [INFO] Successfully imported utils using relative paths.
2025-05-22 13:34:59,692 [INFO] Using 48 CPU cores (75% of available 64).
2025-05-22 13:34:59,693 [INFO] Results will be saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452
2025-05-22 13:34:59,723 [INFO] 
--- Loading Data ---
2025-05-22 13:34:59,772 [INFO] Data Loading & Preprocessing Duration: 0.05 seconds
2025-05-22 13:34:59,772 [INFO] 
--- Starting Model Training ---
2025-05-22 13:34:59,775 [INFO] 
=== Processing model: Random Forest (rf) ===
2025-05-22 13:34:59,777 [INFO] 
--- Running Nested CV for Regression: Random Forest (rf) ---
2025-05-22 13:34:59,777 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/rf
2025-05-22 13:34:59,777 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 13:34:59,777 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 13:34:59,779 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 13:34:59,781 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 13:34:59,781 [INFO] Test set indices range from 100 to 1498
2025-05-22 13:34:59,781 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:34:59,781 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:34:59,783 [INFO] Tree-based model 'rf'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:34:59,783 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:35:26,802 [INFO] Best Params found: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 101}
2025-05-22 13:35:26,802 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0503
2025-05-22 13:35:26,836 [INFO] Hyperparameter Tuning Duration: 27.06 seconds
2025-05-22 13:35:26,836 [INFO] Predicting on outer test set...
2025-05-22 13:35:26,899 [INFO] Calculating performance metrics...
2025-05-22 13:35:26,939 [INFO] Generating diagnostic plots...
2025-05-22 13:35:28,023 [INFO] Diagnostic plots saved.
2025-05-22 13:35:28,023 [INFO] Prediction & Evaluation Duration: 1.19 seconds
2025-05-22 13:35:28,023 [INFO] Calculating and saving SHAP values...
2025-05-22 13:35:28,023 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:35:30,586 [INFO] SHAP Calculation & Saving Duration: 2.56 seconds
2025-05-22 13:35:30,586 [INFO] -- Outer Fold 1 finished. Duration: 30.81 seconds --
2025-05-22 13:35:30,586 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 13:35:30,589 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 13:35:30,589 [INFO] Test set indices range from 50 to 1248
2025-05-22 13:35:30,589 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:35:30,589 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:35:30,591 [INFO] Tree-based model 'rf'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:35:30,591 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:35:57,436 [INFO] Best Params found: {'bootstrap': True, 'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 476}
2025-05-22 13:35:57,437 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0479
2025-05-22 13:35:57,472 [INFO] Hyperparameter Tuning Duration: 26.88 seconds
2025-05-22 13:35:57,472 [INFO] Predicting on outer test set...
2025-05-22 13:35:57,637 [INFO] Calculating performance metrics...
2025-05-22 13:35:57,655 [INFO] Generating diagnostic plots...
2025-05-22 13:35:58,373 [INFO] Diagnostic plots saved.
2025-05-22 13:35:58,373 [INFO] Prediction & Evaluation Duration: 0.90 seconds
2025-05-22 13:35:58,373 [INFO] Calculating and saving SHAP values...
2025-05-22 13:35:58,373 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:37:25,777 [INFO] SHAP Calculation & Saving Duration: 87.40 seconds
2025-05-22 13:37:25,778 [INFO] -- Outer Fold 2 finished. Duration: 115.19 seconds --
2025-05-22 13:37:25,778 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 13:37:25,780 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 13:37:25,780 [INFO] Test set indices range from 0 to 1647
2025-05-22 13:37:25,780 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:37:25,781 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:37:25,782 [INFO] Tree-based model 'rf'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:37:25,785 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:37:51,865 [INFO] Best Params found: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 104}
2025-05-22 13:37:51,867 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0506
2025-05-22 13:37:51,899 [INFO] Hyperparameter Tuning Duration: 26.12 seconds
2025-05-22 13:37:51,899 [INFO] Predicting on outer test set...
2025-05-22 13:37:51,949 [INFO] Calculating performance metrics...
2025-05-22 13:37:51,967 [INFO] Generating diagnostic plots...
2025-05-22 13:37:52,886 [INFO] Diagnostic plots saved.
2025-05-22 13:37:52,886 [INFO] Prediction & Evaluation Duration: 0.99 seconds
2025-05-22 13:37:52,886 [INFO] Calculating and saving SHAP values...
2025-05-22 13:37:52,886 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:37:57,911 [INFO] SHAP Calculation & Saving Duration: 5.03 seconds
2025-05-22 13:37:57,912 [INFO] -- Outer Fold 3 finished. Duration: 32.13 seconds --
2025-05-22 13:37:57,912 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 13:37:57,915 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 13:37:57,915 [INFO] Test set indices range from 200 to 1598
2025-05-22 13:37:57,916 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:37:57,916 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:37:57,918 [INFO] Tree-based model 'rf'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:37:57,919 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:38:24,534 [INFO] Best Params found: {'bootstrap': False, 'max_depth': 30, 'max_features': 'log2', 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 351}
2025-05-22 13:38:24,535 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0273
2025-05-22 13:38:24,542 [INFO] Hyperparameter Tuning Duration: 26.63 seconds
2025-05-22 13:38:24,542 [INFO] Predicting on outer test set...
2025-05-22 13:38:24,666 [INFO] Calculating performance metrics...
2025-05-22 13:38:24,722 [INFO] Generating diagnostic plots...
2025-05-22 13:38:25,394 [INFO] Diagnostic plots saved.
2025-05-22 13:38:25,394 [INFO] Prediction & Evaluation Duration: 0.85 seconds
2025-05-22 13:38:25,394 [INFO] Calculating and saving SHAP values...
2025-05-22 13:38:25,394 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:38:31,621 [INFO] SHAP Calculation & Saving Duration: 6.23 seconds
2025-05-22 13:38:31,621 [INFO] -- Outer Fold 4 finished. Duration: 33.71 seconds --
2025-05-22 13:38:31,621 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 13:38:31,623 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 13:38:31,624 [INFO] Test set indices range from 150 to 1548
2025-05-22 13:38:31,624 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:38:31,624 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:38:31,625 [INFO] Tree-based model 'rf'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:38:31,627 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:38:57,314 [INFO] Best Params found: {'bootstrap': False, 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 8, 'min_samples_split': 5, 'n_estimators': 101}
2025-05-22 13:38:57,314 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0473
2025-05-22 13:38:57,349 [INFO] Hyperparameter Tuning Duration: 25.73 seconds
2025-05-22 13:38:57,349 [INFO] Predicting on outer test set...
2025-05-22 13:38:57,397 [INFO] Calculating performance metrics...
2025-05-22 13:38:57,433 [INFO] Generating diagnostic plots...
2025-05-22 13:38:58,067 [INFO] Diagnostic plots saved.
2025-05-22 13:38:58,067 [INFO] Prediction & Evaluation Duration: 0.72 seconds
2025-05-22 13:38:58,067 [INFO] Calculating and saving SHAP values...
2025-05-22 13:38:58,067 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:39:00,749 [INFO] SHAP Calculation & Saving Duration: 2.68 seconds
2025-05-22 13:39:00,749 [INFO] -- Outer Fold 5 finished. Duration: 29.13 seconds --
2025-05-22 13:39:00,749 [INFO] 
--- Aggregating results for: Random Forest (rf) ---
2025-05-22 13:39:00,750 [INFO] Average Metrics across folds:
2025-05-22 13:39:00,750 [INFO] {
    "r2_mean": 0.7059045567866991,
    "r2_std": 0.19721886139139116,
    "mse_mean": 0.03578803264004103,
    "mse_std": 0.027331785875054974,
    "rmse_mean": 0.17127441749667804,
    "rmse_std": 0.08033123023590927,
    "mae_mean": 0.11630363057428542,
    "mae_std": 0.047515061255243865,
    "best_inner_cv_score_mean": -0.044687305464426966,
    "best_inner_cv_score_std": 0.008784713173027047,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 13:39:00,860 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/rf/all_folds_predictions.csv
2025-05-22 13:39:00,860 [INFO] 
Running global SHAP analysis...
2025-05-22 13:39:02,374 [INFO] Global SHAP Analysis Duration: 1.51 seconds
2025-05-22 13:39:02,374 [INFO] Result Aggregation & Global SHAP Duration: 1.62 seconds
2025-05-22 13:39:02,374 [INFO] Average time per outer fold: 48.19 seconds
2025-05-22 13:39:02,374 [INFO] --- Nested CV completed for: Random Forest (rf) ---
2025-05-22 13:39:02,374 [INFO] Total execution time for model 'rf': 242.60 seconds
2025-05-22 13:39:02,374 [INFO] 
=== Processing model: LightGBM (lgbm) ===
2025-05-22 13:39:02,376 [INFO] 
--- Running Nested CV for Regression: LightGBM (lgbm) ---
2025-05-22 13:39:02,376 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/lgbm
2025-05-22 13:39:02,376 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 13:39:02,376 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 13:39:02,377 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 13:39:02,379 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 13:39:02,379 [INFO] Test set indices range from 100 to 1498
2025-05-22 13:39:02,379 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:39:02,379 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:39:02,383 [INFO] Tree-based model 'lgbm'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:39:02,383 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:47:29,277 [INFO] Best Params found: {'colsample_bytree': 0.6431565707973218, 'learning_rate': 0.01628583713734685, 'max_depth': 20, 'min_child_samples': 26, 'n_estimators': 663, 'num_leaves': 51, 'reg_alpha': 0.6955160864261275, 'reg_lambda': 0.1393314544058757, 'subsample': 0.8417669517111269}
2025-05-22 13:47:29,278 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0582
2025-05-22 13:47:29,286 [INFO] Hyperparameter Tuning Duration: 506.91 seconds
2025-05-22 13:47:29,286 [INFO] Predicting on outer test set...
2025-05-22 13:47:29,292 [INFO] Calculating performance metrics...
2025-05-22 13:47:29,337 [INFO] Generating diagnostic plots...
2025-05-22 13:47:29,994 [INFO] Diagnostic plots saved.
2025-05-22 13:47:29,994 [INFO] Prediction & Evaluation Duration: 0.71 seconds
2025-05-22 13:47:29,994 [INFO] Calculating and saving SHAP values...
2025-05-22 13:47:29,994 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:47:31,779 [INFO] SHAP Calculation & Saving Duration: 1.78 seconds
2025-05-22 13:47:31,779 [INFO] -- Outer Fold 1 finished. Duration: 509.40 seconds --
2025-05-22 13:47:31,779 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 13:47:31,781 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 13:47:31,781 [INFO] Test set indices range from 50 to 1248
2025-05-22 13:47:31,781 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:47:31,782 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:47:31,785 [INFO] Tree-based model 'lgbm'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:47:31,788 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 13:58:16,859 [INFO] Best Params found: {'colsample_bytree': 0.6053059844639466, 'learning_rate': 0.19844035113697056, 'max_depth': 10, 'min_child_samples': 27, 'n_estimators': 876, 'num_leaves': 45, 'reg_alpha': 0.09767211400638387, 'reg_lambda': 0.6842330265121569, 'subsample': 0.7760609974958406}
2025-05-22 13:58:16,860 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0428
2025-05-22 13:58:16,962 [INFO] Hyperparameter Tuning Duration: 645.18 seconds
2025-05-22 13:58:16,962 [INFO] Predicting on outer test set...
2025-05-22 13:58:16,968 [INFO] Calculating performance metrics...
2025-05-22 13:58:17,002 [INFO] Generating diagnostic plots...
2025-05-22 13:58:17,664 [INFO] Diagnostic plots saved.
2025-05-22 13:58:17,664 [INFO] Prediction & Evaluation Duration: 0.70 seconds
2025-05-22 13:58:17,664 [INFO] Calculating and saving SHAP values...
2025-05-22 13:58:17,664 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 13:58:19,423 [INFO] SHAP Calculation & Saving Duration: 1.76 seconds
2025-05-22 13:58:19,423 [INFO] -- Outer Fold 2 finished. Duration: 647.64 seconds --
2025-05-22 13:58:19,423 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 13:58:19,425 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 13:58:19,425 [INFO] Test set indices range from 0 to 1647
2025-05-22 13:58:19,426 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 13:58:19,426 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 13:58:19,429 [INFO] Tree-based model 'lgbm'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 13:58:19,433 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:08:59,602 [INFO] Best Params found: {'colsample_bytree': 0.695824756266789, 'learning_rate': 0.03897897441824462, 'max_depth': 30, 'min_child_samples': 35, 'n_estimators': 771, 'num_leaves': 25, 'reg_alpha': 0.2420552715115004, 'reg_lambda': 0.6721355474058786, 'subsample': 0.9046478461314871}
2025-05-22 14:08:59,602 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0602
2025-05-22 14:08:59,638 [INFO] Hyperparameter Tuning Duration: 640.21 seconds
2025-05-22 14:08:59,638 [INFO] Predicting on outer test set...
2025-05-22 14:08:59,643 [INFO] Calculating performance metrics...
2025-05-22 14:08:59,686 [INFO] Generating diagnostic plots...
2025-05-22 14:09:00,537 [INFO] Diagnostic plots saved.
2025-05-22 14:09:00,537 [INFO] Prediction & Evaluation Duration: 0.90 seconds
2025-05-22 14:09:00,537 [INFO] Calculating and saving SHAP values...
2025-05-22 14:09:00,537 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:09:02,271 [INFO] SHAP Calculation & Saving Duration: 1.73 seconds
2025-05-22 14:09:02,271 [INFO] -- Outer Fold 3 finished. Duration: 642.85 seconds --
2025-05-22 14:09:02,271 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 14:09:02,273 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:09:02,273 [INFO] Test set indices range from 200 to 1598
2025-05-22 14:09:02,274 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:09:02,274 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:09:02,277 [INFO] Tree-based model 'lgbm'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:09:02,280 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:22:51,103 [INFO] Best Params found: {'colsample_bytree': 0.8045369595443751, 'learning_rate': 0.11030325893743992, 'max_depth': -1, 'min_child_samples': 29, 'n_estimators': 420, 'num_leaves': 27, 'reg_alpha': 0.7019668772577033, 'reg_lambda': 0.795792669436101, 'subsample': 0.9560021367270265}
2025-05-22 14:22:51,105 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0254
2025-05-22 14:22:51,161 [INFO] Hyperparameter Tuning Duration: 828.89 seconds
2025-05-22 14:22:51,161 [INFO] Predicting on outer test set...
2025-05-22 14:22:51,170 [INFO] Calculating performance metrics...
2025-05-22 14:22:51,247 [INFO] Generating diagnostic plots...
2025-05-22 14:22:52,253 [INFO] Diagnostic plots saved.
2025-05-22 14:22:52,253 [INFO] Prediction & Evaluation Duration: 1.09 seconds
2025-05-22 14:22:52,253 [INFO] Calculating and saving SHAP values...
2025-05-22 14:22:52,253 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:22:53,536 [INFO] SHAP Calculation & Saving Duration: 1.28 seconds
2025-05-22 14:22:53,536 [INFO] -- Outer Fold 4 finished. Duration: 831.26 seconds --
2025-05-22 14:22:53,536 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 14:22:53,538 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:22:53,538 [INFO] Test set indices range from 150 to 1548
2025-05-22 14:22:53,539 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:22:53,539 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:22:53,543 [INFO] Tree-based model 'lgbm'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:22:53,545 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:32:27,180 [INFO] Best Params found: {'colsample_bytree': 0.8080272084711243, 'learning_rate': 0.11934205586865593, 'max_depth': 10, 'min_child_samples': 38, 'n_estimators': 802, 'num_leaves': 37, 'reg_alpha': 0.7751328233611146, 'reg_lambda': 0.9394989415641891, 'subsample': 0.9579309401710595}
2025-05-22 14:32:27,181 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0437
2025-05-22 14:32:27,208 [INFO] Hyperparameter Tuning Duration: 573.67 seconds
2025-05-22 14:32:27,208 [INFO] Predicting on outer test set...
2025-05-22 14:32:27,212 [INFO] Calculating performance metrics...
2025-05-22 14:32:27,249 [INFO] Generating diagnostic plots...
2025-05-22 14:32:27,887 [INFO] Diagnostic plots saved.
2025-05-22 14:32:27,887 [INFO] Prediction & Evaluation Duration: 0.68 seconds
2025-05-22 14:32:27,887 [INFO] Calculating and saving SHAP values...
2025-05-22 14:32:27,888 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:32:30,066 [INFO] SHAP Calculation & Saving Duration: 2.18 seconds
2025-05-22 14:32:30,066 [INFO] -- Outer Fold 5 finished. Duration: 576.53 seconds --
2025-05-22 14:32:30,066 [INFO] 
--- Aggregating results for: LightGBM (lgbm) ---
2025-05-22 14:32:30,067 [INFO] Average Metrics across folds:
2025-05-22 14:32:30,067 [INFO] {
    "r2_mean": 0.6531234141323023,
    "r2_std": 0.21799428410841895,
    "mse_mean": 0.03905007763817333,
    "mse_std": 0.023627528665876096,
    "rmse_mean": 0.18320117308401584,
    "rmse_std": 0.07407703975466218,
    "mae_mean": 0.11328028414680764,
    "mae_std": 0.044107865034723484,
    "best_inner_cv_score_mean": -0.04606830767418914,
    "best_inner_cv_score_std": 0.01256857970379945,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 14:32:30,163 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/lgbm/all_folds_predictions.csv
2025-05-22 14:32:30,163 [INFO] 
Running global SHAP analysis...
2025-05-22 14:32:31,646 [INFO] Global SHAP Analysis Duration: 1.48 seconds
2025-05-22 14:32:31,646 [INFO] Result Aggregation & Global SHAP Duration: 1.58 seconds
2025-05-22 14:32:31,646 [INFO] Average time per outer fold: 641.54 seconds
2025-05-22 14:32:31,647 [INFO] --- Nested CV completed for: LightGBM (lgbm) ---
2025-05-22 14:32:31,650 [INFO] Total execution time for model 'lgbm': 3209.28 seconds
2025-05-22 14:32:31,650 [INFO] 
=== Processing model: XGBoost (xgb) ===
2025-05-22 14:32:31,651 [INFO] 
--- Running Nested CV for Regression: XGBoost (xgb) ---
2025-05-22 14:32:31,651 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/xgb
2025-05-22 14:32:31,651 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 14:32:31,651 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 14:32:31,652 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 14:32:31,655 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 14:32:31,655 [INFO] Test set indices range from 100 to 1498
2025-05-22 14:32:31,655 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:32:31,655 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:32:31,660 [INFO] Tree-based model 'xgb'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:32:31,660 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:33:52,779 [INFO] Best Params found: {'colsample_bytree': 0.9193380499938204, 'gamma': 0.07535877198271473, 'learning_rate': 0.11163975534814373, 'max_depth': 7, 'n_estimators': 870, 'reg_alpha': 0.5908929431882418, 'reg_lambda': 1.3551287236845648, 'subsample': 0.6066351315711425}
2025-05-22 14:33:52,780 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0517
2025-05-22 14:33:52,811 [INFO] Hyperparameter Tuning Duration: 81.16 seconds
2025-05-22 14:33:52,811 [INFO] Predicting on outer test set...
2025-05-22 14:33:52,819 [INFO] Calculating performance metrics...
2025-05-22 14:33:52,854 [INFO] Generating diagnostic plots...
2025-05-22 14:33:53,596 [INFO] Diagnostic plots saved.
2025-05-22 14:33:53,597 [INFO] Prediction & Evaluation Duration: 0.79 seconds
2025-05-22 14:33:53,597 [INFO] Calculating and saving SHAP values...
2025-05-22 14:33:53,597 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:33:55,058 [INFO] SHAP Calculation & Saving Duration: 1.46 seconds
2025-05-22 14:33:55,058 [INFO] -- Outer Fold 1 finished. Duration: 83.41 seconds --
2025-05-22 14:33:55,058 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 14:33:55,062 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:33:55,062 [INFO] Test set indices range from 50 to 1248
2025-05-22 14:33:55,062 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:33:55,062 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:33:55,066 [INFO] Tree-based model 'xgb'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:33:55,067 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:35:11,260 [INFO] Best Params found: {'colsample_bytree': 0.6421977039321082, 'gamma': 0.22826728524145512, 'learning_rate': 0.05368808744336672, 'max_depth': 6, 'n_estimators': 897, 'reg_alpha': 0.8832802589188683, 'reg_lambda': 0.6486900420105479, 'subsample': 0.6488351818802693}
2025-05-22 14:35:11,267 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0532
2025-05-22 14:35:11,290 [INFO] Hyperparameter Tuning Duration: 76.23 seconds
2025-05-22 14:35:11,290 [INFO] Predicting on outer test set...
2025-05-22 14:35:11,299 [INFO] Calculating performance metrics...
2025-05-22 14:35:11,311 [INFO] Generating diagnostic plots...
2025-05-22 14:35:12,086 [INFO] Diagnostic plots saved.
2025-05-22 14:35:12,086 [INFO] Prediction & Evaluation Duration: 0.80 seconds
2025-05-22 14:35:12,086 [INFO] Calculating and saving SHAP values...
2025-05-22 14:35:12,086 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:35:13,465 [INFO] SHAP Calculation & Saving Duration: 1.38 seconds
2025-05-22 14:35:13,466 [INFO] -- Outer Fold 2 finished. Duration: 78.41 seconds --
2025-05-22 14:35:13,466 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 14:35:13,469 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:35:13,469 [INFO] Test set indices range from 0 to 1647
2025-05-22 14:35:13,469 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:35:13,469 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:35:13,473 [INFO] Tree-based model 'xgb'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:35:13,474 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:36:24,084 [INFO] Best Params found: {'colsample_bytree': 0.6336559859980195, 'gamma': 0.08081435704730688, 'learning_rate': 0.18971083770541586, 'max_depth': 6, 'n_estimators': 353, 'reg_alpha': 0.009197051616629648, 'reg_lambda': 0.2029430857320642, 'subsample': 0.8654007076432223}
2025-05-22 14:36:24,087 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0542
2025-05-22 14:36:24,110 [INFO] Hyperparameter Tuning Duration: 70.64 seconds
2025-05-22 14:36:24,110 [INFO] Predicting on outer test set...
2025-05-22 14:36:24,118 [INFO] Calculating performance metrics...
2025-05-22 14:36:24,175 [INFO] Generating diagnostic plots...
2025-05-22 14:36:25,017 [INFO] Diagnostic plots saved.
2025-05-22 14:36:25,017 [INFO] Prediction & Evaluation Duration: 0.91 seconds
2025-05-22 14:36:25,017 [INFO] Calculating and saving SHAP values...
2025-05-22 14:36:25,017 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:36:26,126 [INFO] SHAP Calculation & Saving Duration: 1.11 seconds
2025-05-22 14:36:26,126 [INFO] -- Outer Fold 3 finished. Duration: 72.66 seconds --
2025-05-22 14:36:26,127 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 14:36:26,130 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:36:26,130 [INFO] Test set indices range from 200 to 1598
2025-05-22 14:36:26,130 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:36:26,130 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:36:26,134 [INFO] Tree-based model 'xgb'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:36:26,135 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:37:40,993 [INFO] Best Params found: {'colsample_bytree': 0.9464704583099741, 'gamma': 0.3005575058716044, 'learning_rate': 0.1516145155592091, 'max_depth': 8, 'n_estimators': 408, 'reg_alpha': 0.9699098521619943, 'reg_lambda': 1.6648852816008435, 'subsample': 0.6849356442713105}
2025-05-22 14:37:40,997 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0281
2025-05-22 14:37:41,037 [INFO] Hyperparameter Tuning Duration: 74.91 seconds
2025-05-22 14:37:41,037 [INFO] Predicting on outer test set...
2025-05-22 14:37:41,045 [INFO] Calculating performance metrics...
2025-05-22 14:37:41,114 [INFO] Generating diagnostic plots...
2025-05-22 14:37:41,828 [INFO] Diagnostic plots saved.
2025-05-22 14:37:41,828 [INFO] Prediction & Evaluation Duration: 0.79 seconds
2025-05-22 14:37:41,828 [INFO] Calculating and saving SHAP values...
2025-05-22 14:37:41,828 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:37:43,029 [INFO] SHAP Calculation & Saving Duration: 1.20 seconds
2025-05-22 14:37:43,029 [INFO] -- Outer Fold 4 finished. Duration: 76.90 seconds --
2025-05-22 14:37:43,030 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 14:37:43,032 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:37:43,032 [INFO] Test set indices range from 150 to 1548
2025-05-22 14:37:43,032 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:37:43,032 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:37:43,036 [INFO] Tree-based model 'xgb'. Setting n_jobs=4 for hyperparameter tuning.
2025-05-22 14:37:43,036 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:01,400 [INFO] Best Params found: {'colsample_bytree': 0.6508242050607539, 'gamma': 0.2611216300274022, 'learning_rate': 0.16399871061972218, 'max_depth': 6, 'n_estimators': 719, 'reg_alpha': 0.6228904758190003, 'reg_lambda': 0.170694929987536, 'subsample': 0.6206726884674431}
2025-05-22 14:39:01,400 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0431
2025-05-22 14:39:01,430 [INFO] Hyperparameter Tuning Duration: 78.40 seconds
2025-05-22 14:39:01,430 [INFO] Predicting on outer test set...
2025-05-22 14:39:01,435 [INFO] Calculating performance metrics...
2025-05-22 14:39:01,469 [INFO] Generating diagnostic plots...
2025-05-22 14:39:02,333 [INFO] Diagnostic plots saved.
2025-05-22 14:39:02,334 [INFO] Prediction & Evaluation Duration: 0.90 seconds
2025-05-22 14:39:02,334 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:02,334 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:03,567 [INFO] SHAP Calculation & Saving Duration: 1.23 seconds
2025-05-22 14:39:03,567 [INFO] -- Outer Fold 5 finished. Duration: 80.54 seconds --
2025-05-22 14:39:03,567 [INFO] 
--- Aggregating results for: XGBoost (xgb) ---
2025-05-22 14:39:03,567 [INFO] Average Metrics across folds:
2025-05-22 14:39:03,567 [INFO] {
    "r2_mean": 0.659061792079853,
    "r2_std": 0.21478966669926147,
    "mse_mean": 0.040616406976905876,
    "mse_std": 0.02820877943187463,
    "rmse_mean": 0.18442514324345702,
    "rmse_std": 0.08126360511653564,
    "mae_mean": 0.11635654830168998,
    "mae_std": 0.04163842759460777,
    "best_inner_cv_score_mean": -0.04605692487007067,
    "best_inner_cv_score_std": 0.009820716118723672,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 14:39:03,626 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/xgb/all_folds_predictions.csv
2025-05-22 14:39:03,627 [INFO] 
Running global SHAP analysis...
2025-05-22 14:39:05,425 [INFO] Global SHAP Analysis Duration: 1.80 seconds
2025-05-22 14:39:05,426 [INFO] Result Aggregation & Global SHAP Duration: 1.86 seconds
2025-05-22 14:39:05,426 [INFO] Average time per outer fold: 78.38 seconds
2025-05-22 14:39:05,426 [INFO] --- Nested CV completed for: XGBoost (xgb) ---
2025-05-22 14:39:05,427 [INFO] Total execution time for model 'xgb': 393.78 seconds
2025-05-22 14:39:05,427 [INFO] 
=== Processing model: Linear Regression (lr) ===
2025-05-22 14:39:05,429 [INFO] 
--- Running Nested CV for Regression: Linear Regression (lr) ---
2025-05-22 14:39:05,429 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/lr
2025-05-22 14:39:05,429 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 14:39:05,429 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 14:39:05,430 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 14:39:05,433 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 14:39:05,433 [INFO] Test set indices range from 100 to 1498
2025-05-22 14:39:05,433 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:05,433 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:05,433 [INFO] Linear model 'lr' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:05,433 [INFO] Parameter space size (2) is smaller than requested n_iter (50). Adjusting to 2.
2025-05-22 14:39:05,433 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:05,600 [INFO] Best Params found: {'reg__fit_intercept': True}
2025-05-22 14:39:05,600 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.1514
2025-05-22 14:39:05,600 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:05,600 [INFO]   - Model type: lr (Linear Regression)
2025-05-22 14:39:05,600 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:05,637 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', LinearRegression())], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': LinearRegression(), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__n_jobs': None, 'reg__positive': False}
2025-05-22 14:39:05,637 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:05,637 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:05,655 [INFO] Hyperparameter Tuning Duration: 0.22 seconds
2025-05-22 14:39:05,655 [INFO] Predicting on outer test set...
2025-05-22 14:39:05,658 [INFO] Calculating performance metrics...
2025-05-22 14:39:05,692 [INFO] Generating diagnostic plots...
2025-05-22 14:39:06,448 [INFO] Diagnostic plots saved.
2025-05-22 14:39:06,448 [INFO] Prediction & Evaluation Duration: 0.79 seconds
2025-05-22 14:39:06,448 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:06,449 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:07,660 [INFO] SHAP Calculation & Saving Duration: 1.21 seconds
2025-05-22 14:39:07,660 [INFO] -- Outer Fold 1 finished. Duration: 2.23 seconds --
2025-05-22 14:39:07,660 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 14:39:07,663 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:07,663 [INFO] Test set indices range from 50 to 1248
2025-05-22 14:39:07,663 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:07,663 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:07,663 [INFO] Linear model 'lr' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:07,663 [INFO] Parameter space size (2) is smaller than requested n_iter (50). Adjusting to 2.
2025-05-22 14:39:07,663 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:07,718 [INFO] Best Params found: {'reg__fit_intercept': True}
2025-05-22 14:39:07,719 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0763
2025-05-22 14:39:07,719 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:07,719 [INFO]   - Model type: lr (Linear Regression)
2025-05-22 14:39:07,719 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:07,719 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', LinearRegression())], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': LinearRegression(), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__n_jobs': None, 'reg__positive': False}
2025-05-22 14:39:07,719 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:07,719 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:07,750 [INFO] Hyperparameter Tuning Duration: 0.09 seconds
2025-05-22 14:39:07,750 [INFO] Predicting on outer test set...
2025-05-22 14:39:07,752 [INFO] Calculating performance metrics...
2025-05-22 14:39:07,805 [INFO] Generating diagnostic plots...
2025-05-22 14:39:08,502 [INFO] Diagnostic plots saved.
2025-05-22 14:39:08,502 [INFO] Prediction & Evaluation Duration: 0.75 seconds
2025-05-22 14:39:08,502 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:08,502 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:09,666 [INFO] SHAP Calculation & Saving Duration: 1.16 seconds
2025-05-22 14:39:09,666 [INFO] -- Outer Fold 2 finished. Duration: 2.01 seconds --
2025-05-22 14:39:09,666 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 14:39:09,668 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:09,668 [INFO] Test set indices range from 0 to 1647
2025-05-22 14:39:09,668 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:09,668 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:09,669 [INFO] Linear model 'lr' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:09,669 [INFO] Parameter space size (2) is smaller than requested n_iter (50). Adjusting to 2.
2025-05-22 14:39:09,669 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:09,725 [INFO] Best Params found: {'reg__fit_intercept': True}
2025-05-22 14:39:09,725 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.1022
2025-05-22 14:39:09,725 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:09,725 [INFO]   - Model type: lr (Linear Regression)
2025-05-22 14:39:09,725 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:09,725 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', LinearRegression())], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': LinearRegression(), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__n_jobs': None, 'reg__positive': False}
2025-05-22 14:39:09,725 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:09,725 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:09,754 [INFO] Hyperparameter Tuning Duration: 0.09 seconds
2025-05-22 14:39:09,755 [INFO] Predicting on outer test set...
2025-05-22 14:39:09,757 [INFO] Calculating performance metrics...
2025-05-22 14:39:09,772 [INFO] Generating diagnostic plots...
2025-05-22 14:39:10,686 [INFO] Diagnostic plots saved.
2025-05-22 14:39:10,686 [INFO] Prediction & Evaluation Duration: 0.93 seconds
2025-05-22 14:39:10,686 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:10,686 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:11,924 [INFO] SHAP Calculation & Saving Duration: 1.24 seconds
2025-05-22 14:39:11,924 [INFO] -- Outer Fold 3 finished. Duration: 2.26 seconds --
2025-05-22 14:39:11,924 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 14:39:11,926 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:39:11,926 [INFO] Test set indices range from 200 to 1598
2025-05-22 14:39:11,926 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:11,926 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:11,926 [INFO] Linear model 'lr' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:11,926 [INFO] Parameter space size (2) is smaller than requested n_iter (50). Adjusting to 2.
2025-05-22 14:39:11,927 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:11,982 [INFO] Best Params found: {'reg__fit_intercept': True}
2025-05-22 14:39:11,982 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0670
2025-05-22 14:39:11,982 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:11,982 [INFO]   - Model type: lr (Linear Regression)
2025-05-22 14:39:11,982 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:11,983 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', LinearRegression())], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': LinearRegression(), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__n_jobs': None, 'reg__positive': False}
2025-05-22 14:39:11,983 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:11,983 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:12,009 [INFO] Hyperparameter Tuning Duration: 0.08 seconds
2025-05-22 14:39:12,009 [INFO] Predicting on outer test set...
2025-05-22 14:39:12,011 [INFO] Calculating performance metrics...
2025-05-22 14:39:12,091 [INFO] Generating diagnostic plots...
2025-05-22 14:39:12,846 [INFO] Diagnostic plots saved.
2025-05-22 14:39:12,846 [INFO] Prediction & Evaluation Duration: 0.84 seconds
2025-05-22 14:39:12,846 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:12,846 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:13,963 [INFO] SHAP Calculation & Saving Duration: 1.12 seconds
2025-05-22 14:39:13,964 [INFO] -- Outer Fold 4 finished. Duration: 2.04 seconds --
2025-05-22 14:39:13,964 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 14:39:13,966 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:39:13,966 [INFO] Test set indices range from 150 to 1548
2025-05-22 14:39:13,966 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:13,966 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:13,966 [INFO] Linear model 'lr' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:13,966 [INFO] Parameter space size (2) is smaller than requested n_iter (50). Adjusting to 2.
2025-05-22 14:39:13,966 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:14,023 [INFO] Best Params found: {'reg__fit_intercept': True}
2025-05-22 14:39:14,023 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.1165
2025-05-22 14:39:14,023 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:14,023 [INFO]   - Model type: lr (Linear Regression)
2025-05-22 14:39:14,023 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:14,023 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', LinearRegression())], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': LinearRegression(), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__n_jobs': None, 'reg__positive': False}
2025-05-22 14:39:14,023 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:14,023 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:14,055 [INFO] Hyperparameter Tuning Duration: 0.09 seconds
2025-05-22 14:39:14,055 [INFO] Predicting on outer test set...
2025-05-22 14:39:14,057 [INFO] Calculating performance metrics...
2025-05-22 14:39:14,091 [INFO] Generating diagnostic plots...
2025-05-22 14:39:15,044 [INFO] Diagnostic plots saved.
2025-05-22 14:39:15,045 [INFO] Prediction & Evaluation Duration: 0.99 seconds
2025-05-22 14:39:15,045 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:15,045 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:16,284 [INFO] SHAP Calculation & Saving Duration: 1.24 seconds
2025-05-22 14:39:16,284 [INFO] -- Outer Fold 5 finished. Duration: 2.32 seconds --
2025-05-22 14:39:16,284 [INFO] 
--- Aggregating results for: Linear Regression (lr) ---
2025-05-22 14:39:16,285 [INFO] Average Metrics across folds:
2025-05-22 14:39:16,285 [INFO] {
    "r2_mean": 0.49778787200632524,
    "r2_std": 0.12572988617290726,
    "mse_mean": 0.060010889228860456,
    "mse_std": 0.021948693301216717,
    "rmse_mean": 0.24083667158847105,
    "rmse_std": 0.04481726059284963,
    "mae_mean": 0.16796271407141458,
    "mae_std": 0.010923788388581289,
    "best_inner_cv_score_mean": -0.10267378776084454,
    "best_inner_cv_score_std": 0.030100024358990528,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 14:39:16,344 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/lr/all_folds_predictions.csv
2025-05-22 14:39:16,344 [INFO] 
Running global SHAP analysis...
2025-05-22 14:39:18,006 [INFO] Global SHAP Analysis Duration: 1.66 seconds
2025-05-22 14:39:18,006 [INFO] Result Aggregation & Global SHAP Duration: 1.72 seconds
2025-05-22 14:39:18,006 [INFO] Average time per outer fold: 2.17 seconds
2025-05-22 14:39:18,006 [INFO] --- Nested CV completed for: Linear Regression (lr) ---
2025-05-22 14:39:18,006 [INFO] Total execution time for model 'lr': 12.58 seconds
2025-05-22 14:39:18,006 [INFO] 
=== Processing model: Ridge Regression (ridge) ===
2025-05-22 14:39:18,008 [INFO] 
--- Running Nested CV for Regression: Ridge Regression (ridge) ---
2025-05-22 14:39:18,008 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/ridge
2025-05-22 14:39:18,008 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 14:39:18,008 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 14:39:18,009 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 14:39:18,012 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 14:39:18,012 [INFO] Test set indices range from 100 to 1498
2025-05-22 14:39:18,012 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:18,012 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:18,013 [INFO] Linear model 'ridge' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:18,013 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:19,665 [INFO] Best Params found: {'reg__alpha': 676.5074324464838, 'reg__fit_intercept': True, 'reg__solver': 'auto'}
2025-05-22 14:39:19,665 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0369
2025-05-22 14:39:19,665 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:19,665 [INFO]   - Model type: ridge (Ridge Regression)
2025-05-22 14:39:19,665 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:19,666 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Ridge(alpha=676.5074324464838))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Ridge(alpha=676.5074324464838), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 676.5074324464838, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': None, 'reg__positive': False, 'reg__random_state': None, 'reg__solver': 'auto', 'reg__tol': 0.0001}
2025-05-22 14:39:19,666 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:19,666 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:19,666 [INFO]   - Alpha value: 676.5074324464838
2025-05-22 14:39:19,708 [INFO] Hyperparameter Tuning Duration: 1.70 seconds
2025-05-22 14:39:19,708 [INFO] Predicting on outer test set...
2025-05-22 14:39:19,709 [INFO] Calculating performance metrics...
2025-05-22 14:39:19,768 [INFO] Generating diagnostic plots...
2025-05-22 14:39:20,626 [INFO] Diagnostic plots saved.
2025-05-22 14:39:20,626 [INFO] Prediction & Evaluation Duration: 0.92 seconds
2025-05-22 14:39:20,626 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:20,626 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:21,922 [INFO] SHAP Calculation & Saving Duration: 1.30 seconds
2025-05-22 14:39:21,922 [INFO] -- Outer Fold 1 finished. Duration: 3.91 seconds --
2025-05-22 14:39:21,922 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 14:39:21,924 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:21,924 [INFO] Test set indices range from 50 to 1248
2025-05-22 14:39:21,924 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:21,924 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:21,925 [INFO] Linear model 'ridge' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:21,925 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:23,136 [INFO] Best Params found: {'reg__alpha': 308.85742992785003, 'reg__fit_intercept': True, 'reg__solver': 'lsqr'}
2025-05-22 14:39:23,136 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0236
2025-05-22 14:39:23,136 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:23,136 [INFO]   - Model type: ridge (Ridge Regression)
2025-05-22 14:39:23,136 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:23,137 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Ridge(alpha=308.85742992785003, solver='lsqr'))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Ridge(alpha=308.85742992785003, solver='lsqr'), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 308.85742992785003, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': None, 'reg__positive': False, 'reg__random_state': None, 'reg__solver': 'lsqr', 'reg__tol': 0.0001}
2025-05-22 14:39:23,137 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:23,137 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:23,137 [INFO]   - Alpha value: 308.85742992785003
2025-05-22 14:39:23,179 [INFO] Hyperparameter Tuning Duration: 1.25 seconds
2025-05-22 14:39:23,179 [INFO] Predicting on outer test set...
2025-05-22 14:39:23,181 [INFO] Calculating performance metrics...
2025-05-22 14:39:23,215 [INFO] Generating diagnostic plots...
2025-05-22 14:39:24,055 [INFO] Diagnostic plots saved.
2025-05-22 14:39:24,055 [INFO] Prediction & Evaluation Duration: 0.88 seconds
2025-05-22 14:39:24,055 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:24,055 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:25,217 [INFO] SHAP Calculation & Saving Duration: 1.16 seconds
2025-05-22 14:39:25,217 [INFO] -- Outer Fold 2 finished. Duration: 3.30 seconds --
2025-05-22 14:39:25,217 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 14:39:25,220 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:25,220 [INFO] Test set indices range from 0 to 1647
2025-05-22 14:39:25,220 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:25,220 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:25,220 [INFO] Linear model 'ridge' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:25,221 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:26,307 [INFO] Best Params found: {'reg__alpha': 676.5074324464838, 'reg__fit_intercept': True, 'reg__solver': 'auto'}
2025-05-22 14:39:26,307 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0549
2025-05-22 14:39:26,307 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:26,307 [INFO]   - Model type: ridge (Ridge Regression)
2025-05-22 14:39:26,307 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:26,307 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Ridge(alpha=676.5074324464838))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Ridge(alpha=676.5074324464838), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 676.5074324464838, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': None, 'reg__positive': False, 'reg__random_state': None, 'reg__solver': 'auto', 'reg__tol': 0.0001}
2025-05-22 14:39:26,307 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:26,307 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:26,307 [INFO]   - Alpha value: 676.5074324464838
2025-05-22 14:39:26,347 [INFO] Hyperparameter Tuning Duration: 1.13 seconds
2025-05-22 14:39:26,347 [INFO] Predicting on outer test set...
2025-05-22 14:39:26,349 [INFO] Calculating performance metrics...
2025-05-22 14:39:26,431 [INFO] Generating diagnostic plots...
2025-05-22 14:39:27,148 [INFO] Diagnostic plots saved.
2025-05-22 14:39:27,148 [INFO] Prediction & Evaluation Duration: 0.80 seconds
2025-05-22 14:39:27,148 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:27,148 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:28,314 [INFO] SHAP Calculation & Saving Duration: 1.17 seconds
2025-05-22 14:39:28,314 [INFO] -- Outer Fold 3 finished. Duration: 3.10 seconds --
2025-05-22 14:39:28,314 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 14:39:28,317 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:39:28,317 [INFO] Test set indices range from 200 to 1598
2025-05-22 14:39:28,317 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:28,317 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:28,318 [INFO] Linear model 'ridge' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:28,318 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:29,417 [INFO] Best Params found: {'reg__alpha': 676.5074324464838, 'reg__fit_intercept': True, 'reg__solver': 'auto'}
2025-05-22 14:39:29,417 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0262
2025-05-22 14:39:29,417 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:29,418 [INFO]   - Model type: ridge (Ridge Regression)
2025-05-22 14:39:29,418 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:29,418 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Ridge(alpha=676.5074324464838))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Ridge(alpha=676.5074324464838), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 676.5074324464838, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': None, 'reg__positive': False, 'reg__random_state': None, 'reg__solver': 'auto', 'reg__tol': 0.0001}
2025-05-22 14:39:29,418 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:29,418 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:29,418 [INFO]   - Alpha value: 676.5074324464838
2025-05-22 14:39:29,447 [INFO] Hyperparameter Tuning Duration: 1.13 seconds
2025-05-22 14:39:29,447 [INFO] Predicting on outer test set...
2025-05-22 14:39:29,449 [INFO] Calculating performance metrics...
2025-05-22 14:39:29,499 [INFO] Generating diagnostic plots...
2025-05-22 14:39:30,438 [INFO] Diagnostic plots saved.
2025-05-22 14:39:30,438 [INFO] Prediction & Evaluation Duration: 0.99 seconds
2025-05-22 14:39:30,438 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:30,438 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:31,513 [INFO] SHAP Calculation & Saving Duration: 1.07 seconds
2025-05-22 14:39:31,513 [INFO] -- Outer Fold 4 finished. Duration: 3.20 seconds --
2025-05-22 14:39:31,513 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 14:39:31,515 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:39:31,515 [INFO] Test set indices range from 150 to 1548
2025-05-22 14:39:31,515 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:31,515 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:31,516 [INFO] Linear model 'ridge' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:31,516 [INFO] Fitting with groups parameter for GroupKFold
2025-05-22 14:39:32,587 [INFO] Best Params found: {'reg__alpha': 676.5074324464838, 'reg__fit_intercept': True, 'reg__solver': 'auto'}
2025-05-22 14:39:32,587 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0412
2025-05-22 14:39:32,587 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:32,587 [INFO]   - Model type: ridge (Ridge Regression)
2025-05-22 14:39:32,587 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:32,588 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Ridge(alpha=676.5074324464838))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Ridge(alpha=676.5074324464838), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 676.5074324464838, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': None, 'reg__positive': False, 'reg__random_state': None, 'reg__solver': 'auto', 'reg__tol': 0.0001}
2025-05-22 14:39:32,588 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:32,588 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:32,588 [INFO]   - Alpha value: 676.5074324464838
2025-05-22 14:39:32,620 [INFO] Hyperparameter Tuning Duration: 1.10 seconds
2025-05-22 14:39:32,620 [INFO] Predicting on outer test set...
2025-05-22 14:39:32,622 [INFO] Calculating performance metrics...
2025-05-22 14:39:32,645 [INFO] Generating diagnostic plots...
2025-05-22 14:39:33,214 [INFO] Diagnostic plots saved.
2025-05-22 14:39:33,214 [INFO] Prediction & Evaluation Duration: 0.59 seconds
2025-05-22 14:39:33,214 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:33,214 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:34,367 [INFO] SHAP Calculation & Saving Duration: 1.15 seconds
2025-05-22 14:39:34,367 [INFO] -- Outer Fold 5 finished. Duration: 2.85 seconds --
2025-05-22 14:39:34,367 [INFO] 
--- Aggregating results for: Ridge Regression (ridge) ---
2025-05-22 14:39:34,368 [INFO] Average Metrics across folds:
2025-05-22 14:39:34,368 [INFO] {
    "r2_mean": 0.7280408987879299,
    "r2_std": 0.16028989822831097,
    "mse_mean": 0.03382644999113027,
    "mse_std": 0.023901384180692833,
    "rmse_mean": 0.16988943420906444,
    "rmse_std": 0.07045587367462157,
    "mae_mean": 0.11623280898218827,
    "mae_std": 0.041415403173608804,
    "best_inner_cv_score_mean": -0.03655004958546497,
    "best_inner_cv_score_std": 0.011260277078077861,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 14:39:34,448 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/ridge/all_folds_predictions.csv
2025-05-22 14:39:34,448 [INFO] 
Running global SHAP analysis...
2025-05-22 14:39:36,212 [INFO] Global SHAP Analysis Duration: 1.76 seconds
2025-05-22 14:39:36,212 [INFO] Result Aggregation & Global SHAP Duration: 1.84 seconds
2025-05-22 14:39:36,212 [INFO] Average time per outer fold: 3.27 seconds
2025-05-22 14:39:36,212 [INFO] --- Nested CV completed for: Ridge Regression (ridge) ---
2025-05-22 14:39:36,213 [INFO] Total execution time for model 'ridge': 18.21 seconds
2025-05-22 14:39:36,213 [INFO] 
=== Processing model: Lasso Regression (lasso) ===
2025-05-22 14:39:36,214 [INFO] 
--- Running Nested CV for Regression: Lasso Regression (lasso) ---
2025-05-22 14:39:36,214 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/lasso
2025-05-22 14:39:36,214 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 14:39:36,214 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 14:39:36,215 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 14:39:36,218 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 14:39:36,218 [INFO] Test set indices range from 100 to 1498
2025-05-22 14:39:36,218 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:36,218 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:36,219 [INFO] Linear model 'lasso' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:36,219 [INFO] Parameter space size (40) is smaller than requested n_iter (50). Adjusting to 40.
2025-05-22 14:39:36,219 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.670e-02, tolerance: 1.153e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.284e-01, tolerance: 8.416e-03
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:37,140 [INFO] Best Params found: {'reg__alpha': 0.03334792728637585, 'reg__fit_intercept': True, 'reg__max_iter': 3000}
2025-05-22 14:39:37,140 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0385
2025-05-22 14:39:37,140 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:37,140 [INFO]   - Model type: lasso (Lasso Regression)
2025-05-22 14:39:37,140 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:37,141 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Lasso(alpha=0.03334792728637585, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Lasso(alpha=0.03334792728637585, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.03334792728637585, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:37,141 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:37,141 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:37,141 [INFO]   - Alpha value: 0.03334792728637585
2025-05-22 14:39:37,164 [INFO] Hyperparameter Tuning Duration: 0.95 seconds
2025-05-22 14:39:37,164 [INFO] Predicting on outer test set...
2025-05-22 14:39:37,166 [INFO] Calculating performance metrics...
2025-05-22 14:39:37,249 [INFO] Generating diagnostic plots...
2025-05-22 14:39:38,018 [INFO] Diagnostic plots saved.
2025-05-22 14:39:38,018 [INFO] Prediction & Evaluation Duration: 0.85 seconds
2025-05-22 14:39:38,019 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:38,019 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:39,268 [INFO] SHAP Calculation & Saving Duration: 1.25 seconds
2025-05-22 14:39:39,268 [INFO] -- Outer Fold 1 finished. Duration: 3.05 seconds --
2025-05-22 14:39:39,268 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 14:39:39,270 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:39,270 [INFO] Test set indices range from 50 to 1248
2025-05-22 14:39:39,271 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:39,271 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:39,271 [INFO] Linear model 'lasso' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:39,271 [INFO] Parameter space size (40) is smaller than requested n_iter (50). Adjusting to 40.
2025-05-22 14:39:39,272 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.030e-02, tolerance: 1.568e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.200e-02, tolerance: 1.049e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.972e-02, tolerance: 1.302e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:40,373 [INFO] Best Params found: {'reg__alpha': 0.00407559644007287, 'reg__fit_intercept': True, 'reg__max_iter': 3000}
2025-05-22 14:39:40,373 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0261
2025-05-22 14:39:40,373 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:40,374 [INFO]   - Model type: lasso (Lasso Regression)
2025-05-22 14:39:40,374 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:40,374 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Lasso(alpha=0.00407559644007287, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Lasso(alpha=0.00407559644007287, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.00407559644007287, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:40,374 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:40,374 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:40,374 [INFO]   - Alpha value: 0.00407559644007287
2025-05-22 14:39:40,426 [INFO] Hyperparameter Tuning Duration: 1.16 seconds
2025-05-22 14:39:40,426 [INFO] Predicting on outer test set...
2025-05-22 14:39:40,429 [INFO] Calculating performance metrics...
2025-05-22 14:39:40,446 [INFO] Generating diagnostic plots...
2025-05-22 14:39:41,239 [INFO] Diagnostic plots saved.
2025-05-22 14:39:41,239 [INFO] Prediction & Evaluation Duration: 0.81 seconds
2025-05-22 14:39:41,239 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:41,239 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:42,467 [INFO] SHAP Calculation & Saving Duration: 1.23 seconds
2025-05-22 14:39:42,467 [INFO] -- Outer Fold 2 finished. Duration: 3.20 seconds --
2025-05-22 14:39:42,467 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 14:39:42,470 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:42,470 [INFO] Test set indices range from 0 to 1647
2025-05-22 14:39:42,470 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:42,470 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:42,471 [INFO] Linear model 'lasso' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:42,471 [INFO] Parameter space size (40) is smaller than requested n_iter (50). Adjusting to 40.
2025-05-22 14:39:42,471 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.319e-02, tolerance: 2.003e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e-01, tolerance: 1.213e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e+00, tolerance: 1.398e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:43,587 [INFO] Best Params found: {'reg__alpha': 0.03334792728637585, 'reg__fit_intercept': True, 'reg__max_iter': 3000}
2025-05-22 14:39:43,587 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0512
2025-05-22 14:39:43,587 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:43,587 [INFO]   - Model type: lasso (Lasso Regression)
2025-05-22 14:39:43,587 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:43,587 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Lasso(alpha=0.03334792728637585, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Lasso(alpha=0.03334792728637585, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.03334792728637585, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:43,587 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:43,587 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:43,587 [INFO]   - Alpha value: 0.03334792728637585
2025-05-22 14:39:43,615 [INFO] Hyperparameter Tuning Duration: 1.15 seconds
2025-05-22 14:39:43,615 [INFO] Predicting on outer test set...
2025-05-22 14:39:43,618 [INFO] Calculating performance metrics...
2025-05-22 14:39:43,668 [INFO] Generating diagnostic plots...
2025-05-22 14:39:44,439 [INFO] Diagnostic plots saved.
2025-05-22 14:39:44,439 [INFO] Prediction & Evaluation Duration: 0.82 seconds
2025-05-22 14:39:44,439 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:44,439 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:45,639 [INFO] SHAP Calculation & Saving Duration: 1.20 seconds
2025-05-22 14:39:45,639 [INFO] -- Outer Fold 3 finished. Duration: 3.17 seconds --
2025-05-22 14:39:45,640 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 14:39:45,642 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:39:45,642 [INFO] Test set indices range from 200 to 1598
2025-05-22 14:39:45,642 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:45,642 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:45,643 [INFO] Linear model 'lasso' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:45,643 [INFO] Parameter space size (40) is smaller than requested n_iter (50). Adjusting to 40.
2025-05-22 14:39:45,643 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.881e-02, tolerance: 2.481e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.231e-02, tolerance: 9.834e-03
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.927e-02, tolerance: 1.374e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.329e-01, tolerance: 1.372e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:47,001 [INFO] Best Params found: {'reg__alpha': 0.009962513222055111, 'reg__fit_intercept': True, 'reg__max_iter': 3000}
2025-05-22 14:39:47,001 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0292
2025-05-22 14:39:47,001 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:47,001 [INFO]   - Model type: lasso (Lasso Regression)
2025-05-22 14:39:47,001 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:47,001 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Lasso(alpha=0.009962513222055111, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Lasso(alpha=0.009962513222055111, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.009962513222055111, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:47,001 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:47,001 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:47,001 [INFO]   - Alpha value: 0.009962513222055111
2025-05-22 14:39:47,038 [INFO] Hyperparameter Tuning Duration: 1.40 seconds
2025-05-22 14:39:47,038 [INFO] Predicting on outer test set...
2025-05-22 14:39:47,040 [INFO] Calculating performance metrics...
2025-05-22 14:39:47,071 [INFO] Generating diagnostic plots...
2025-05-22 14:39:47,762 [INFO] Diagnostic plots saved.
2025-05-22 14:39:47,763 [INFO] Prediction & Evaluation Duration: 0.72 seconds
2025-05-22 14:39:47,763 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:47,763 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:48,848 [INFO] SHAP Calculation & Saving Duration: 1.09 seconds
2025-05-22 14:39:48,848 [INFO] -- Outer Fold 4 finished. Duration: 3.21 seconds --
2025-05-22 14:39:48,848 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 14:39:48,851 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:39:48,851 [INFO] Test set indices range from 150 to 1548
2025-05-22 14:39:48,851 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:48,851 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:48,852 [INFO] Linear model 'lasso' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:48,852 [INFO] Parameter space size (40) is smaller than requested n_iter (50). Adjusting to 40.
2025-05-22 14:39:48,852 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.175e-01, tolerance: 1.799e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.281e-02, tolerance: 2.641e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.072e-02, tolerance: 1.799e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e-01, tolerance: 1.089e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.897e-02, tolerance: 1.385e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.650e-01, tolerance: 1.418e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:50,042 [INFO] Best Params found: {'reg__alpha': 0.03334792728637585, 'reg__fit_intercept': True, 'reg__max_iter': 3000}
2025-05-22 14:39:50,042 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0370
2025-05-22 14:39:50,042 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:50,042 [INFO]   - Model type: lasso (Lasso Regression)
2025-05-22 14:39:50,042 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:50,043 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', Lasso(alpha=0.03334792728637585, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': Lasso(alpha=0.03334792728637585, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.03334792728637585, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:50,043 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:50,043 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:50,043 [INFO]   - Alpha value: 0.03334792728637585
2025-05-22 14:39:50,073 [INFO] Hyperparameter Tuning Duration: 1.22 seconds
2025-05-22 14:39:50,073 [INFO] Predicting on outer test set...
2025-05-22 14:39:50,074 [INFO] Calculating performance metrics...
2025-05-22 14:39:50,106 [INFO] Generating diagnostic plots...
2025-05-22 14:39:50,836 [INFO] Diagnostic plots saved.
2025-05-22 14:39:50,836 [INFO] Prediction & Evaluation Duration: 0.76 seconds
2025-05-22 14:39:50,836 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:50,837 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:52,143 [INFO] SHAP Calculation & Saving Duration: 1.31 seconds
2025-05-22 14:39:52,143 [INFO] -- Outer Fold 5 finished. Duration: 3.29 seconds --
2025-05-22 14:39:52,143 [INFO] 
--- Aggregating results for: Lasso Regression (lasso) ---
2025-05-22 14:39:52,143 [INFO] Average Metrics across folds:
2025-05-22 14:39:52,144 [INFO] {
    "r2_mean": 0.7241956681147566,
    "r2_std": 0.17599114319118608,
    "mse_mean": 0.03466519209589708,
    "mse_std": 0.026607041215633374,
    "rmse_mean": 0.16998461750973554,
    "rmse_std": 0.07596329314850686,
    "mae_mean": 0.12048575712609047,
    "mae_std": 0.0427501519922506,
    "best_inner_cv_score_mean": -0.03638614749585865,
    "best_inner_cv_score_std": 0.008732391703187145,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 14:39:52,210 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/lasso/all_folds_predictions.csv
2025-05-22 14:39:52,211 [INFO] 
Running global SHAP analysis...
2025-05-22 14:39:53,788 [INFO] Global SHAP Analysis Duration: 1.58 seconds
2025-05-22 14:39:53,788 [INFO] Result Aggregation & Global SHAP Duration: 1.64 seconds
2025-05-22 14:39:53,788 [INFO] Average time per outer fold: 3.19 seconds
2025-05-22 14:39:53,788 [INFO] --- Nested CV completed for: Lasso Regression (lasso) ---
2025-05-22 14:39:53,788 [INFO] Total execution time for model 'lasso': 17.58 seconds
2025-05-22 14:39:53,788 [INFO] 
=== Processing model: Elastic Net (en) ===
2025-05-22 14:39:53,790 [INFO] 
--- Running Nested CV for Regression: Elastic Net (en) ---
2025-05-22 14:39:53,790 [INFO] Output directory: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/en
2025-05-22 14:39:53,790 [INFO] Hyperparameter tuning scoring metric: neg_mean_squared_error
2025-05-22 14:39:53,790 [INFO] Using GroupKFold for outer CV with 5 folds based on query IDs.
2025-05-22 14:39:53,791 [INFO] 
-- Processing Outer Fold 1/5 --
2025-05-22 14:39:53,793 [INFO] Train set size: (1298, 61), Test set size: (350, 61)
2025-05-22 14:39:53,794 [INFO] Test set indices range from 100 to 1498
2025-05-22 14:39:53,794 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:53,794 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:53,794 [INFO] Linear model 'en' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:53,794 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.363e+01, tolerance: 2.060e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e-02, tolerance: 1.744e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.698e+00, tolerance: 1.111e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.184e+01, tolerance: 2.060e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.569e-02, tolerance: 1.744e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.816e+00, tolerance: 1.111e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.227e-01, tolerance: 2.060e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e-02, tolerance: 1.111e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.507e-01, tolerance: 1.153e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e-01, tolerance: 8.416e-03
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:55,579 [INFO] Best Params found: {'reg__alpha': 0.0745934328572655, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.5, 'reg__max_iter': 3000}
2025-05-22 14:39:55,579 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0366
2025-05-22 14:39:55,579 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:55,579 [INFO]   - Model type: en (Elastic Net)
2025-05-22 14:39:55,579 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:55,580 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', ElasticNet(alpha=0.0745934328572655, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': ElasticNet(alpha=0.0745934328572655, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.0745934328572655, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.5, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:55,580 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:55,580 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:55,580 [INFO]   - Alpha value: 0.0745934328572655
2025-05-22 14:39:55,598 [INFO] Hyperparameter Tuning Duration: 1.80 seconds
2025-05-22 14:39:55,599 [INFO] Predicting on outer test set...
2025-05-22 14:39:55,601 [INFO] Calculating performance metrics...
2025-05-22 14:39:55,651 [INFO] Generating diagnostic plots...
2025-05-22 14:39:56,363 [INFO] Diagnostic plots saved.
2025-05-22 14:39:56,363 [INFO] Prediction & Evaluation Duration: 0.76 seconds
2025-05-22 14:39:56,363 [INFO] Calculating and saving SHAP values...
2025-05-22 14:39:56,363 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:39:57,552 [INFO] SHAP Calculation & Saving Duration: 1.19 seconds
2025-05-22 14:39:57,552 [INFO] -- Outer Fold 1 finished. Duration: 3.76 seconds --
2025-05-22 14:39:57,553 [INFO] 
-- Processing Outer Fold 2/5 --
2025-05-22 14:39:57,555 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:39:57,555 [INFO] Test set indices range from 50 to 1248
2025-05-22 14:39:57,555 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:39:57,555 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:39:57,556 [INFO] Linear model 'en' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:39:57,556 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.038e+00, tolerance: 1.568e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.432e-01, tolerance: 1.476e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.360e-01, tolerance: 2.478e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.270e+00, tolerance: 1.568e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e+00, tolerance: 1.476e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.210e+00, tolerance: 2.478e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.994e-01, tolerance: 2.478e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.624e-02, tolerance: 1.049e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.364e-01, tolerance: 1.302e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:39:59,373 [INFO] Best Params found: {'reg__alpha': 0.0302947608528071, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.1, 'reg__max_iter': 1000}
2025-05-22 14:39:59,373 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0244
2025-05-22 14:39:59,373 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:39:59,373 [INFO]   - Model type: en (Elastic Net)
2025-05-22 14:39:59,373 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:39:59,374 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', ElasticNet(alpha=0.0302947608528071, l1_ratio=0.1))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': ElasticNet(alpha=0.0302947608528071, l1_ratio=0.1), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.0302947608528071, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.1, 'reg__max_iter': 1000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:39:59,374 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:39:59,374 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:39:59,374 [INFO]   - Alpha value: 0.0302947608528071
2025-05-22 14:39:59,406 [INFO] Hyperparameter Tuning Duration: 1.85 seconds
2025-05-22 14:39:59,406 [INFO] Predicting on outer test set...
2025-05-22 14:39:59,409 [INFO] Calculating performance metrics...
2025-05-22 14:39:59,441 [INFO] Generating diagnostic plots...
2025-05-22 14:40:00,104 [INFO] Diagnostic plots saved.
2025-05-22 14:40:00,104 [INFO] Prediction & Evaluation Duration: 0.70 seconds
2025-05-22 14:40:00,104 [INFO] Calculating and saving SHAP values...
2025-05-22 14:40:00,104 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:40:01,812 [INFO] SHAP Calculation & Saving Duration: 1.71 seconds
2025-05-22 14:40:01,812 [INFO] -- Outer Fold 2 finished. Duration: 4.26 seconds --
2025-05-22 14:40:01,813 [INFO] 
-- Processing Outer Fold 3/5 --
2025-05-22 14:40:01,814 [INFO] Train set size: (1299, 61), Test set size: (349, 61)
2025-05-22 14:40:01,815 [INFO] Test set indices range from 0 to 1647
2025-05-22 14:40:01,815 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:40:01,815 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:40:01,815 [INFO] Linear model 'en' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:40:01,815 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.296e-01, tolerance: 1.858e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.066e+00, tolerance: 2.003e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.163e+01, tolerance: 2.648e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.536e+00, tolerance: 1.858e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e+01, tolerance: 2.003e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.374e+01, tolerance: 2.648e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.356e-02, tolerance: 2.003e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.399e+00, tolerance: 2.648e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.978e-02, tolerance: 1.213e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.136e-01, tolerance: 1.398e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.818e-02, tolerance: 2.648e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:40:03,699 [INFO] Best Params found: {'reg__alpha': 0.0487613652796118, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.5, 'reg__max_iter': 3000}
2025-05-22 14:40:03,700 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0486
2025-05-22 14:40:03,700 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:40:03,700 [INFO]   - Model type: en (Elastic Net)
2025-05-22 14:40:03,700 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:40:03,700 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', ElasticNet(alpha=0.0487613652796118, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': ElasticNet(alpha=0.0487613652796118, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.0487613652796118, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.5, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:40:03,700 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:40:03,700 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:40:03,700 [INFO]   - Alpha value: 0.0487613652796118
2025-05-22 14:40:03,706 [INFO] Hyperparameter Tuning Duration: 1.89 seconds
2025-05-22 14:40:03,706 [INFO] Predicting on outer test set...
2025-05-22 14:40:03,708 [INFO] Calculating performance metrics...
2025-05-22 14:40:03,722 [INFO] Generating diagnostic plots...
2025-05-22 14:40:04,384 [INFO] Diagnostic plots saved.
2025-05-22 14:40:04,384 [INFO] Prediction & Evaluation Duration: 0.68 seconds
2025-05-22 14:40:04,384 [INFO] Calculating and saving SHAP values...
2025-05-22 14:40:04,384 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:40:05,511 [INFO] SHAP Calculation & Saving Duration: 1.13 seconds
2025-05-22 14:40:05,512 [INFO] -- Outer Fold 3 finished. Duration: 3.70 seconds --
2025-05-22 14:40:05,512 [INFO] 
-- Processing Outer Fold 4/5 --
2025-05-22 14:40:05,514 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:40:05,514 [INFO] Test set indices range from 200 to 1598
2025-05-22 14:40:05,514 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:40:05,514 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:40:05,515 [INFO] Linear model 'en' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:40:05,515 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.994e+00, tolerance: 1.521e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e+00, tolerance: 2.626e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.792e+01, tolerance: 2.481e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.284e+00, tolerance: 1.521e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.052e+00, tolerance: 2.626e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.117e+01, tolerance: 2.481e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.172e-01, tolerance: 1.521e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.945e-02, tolerance: 2.626e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.801e-01, tolerance: 2.481e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e-02, tolerance: 1.374e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.922e-02, tolerance: 9.834e-03
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.418e-01, tolerance: 1.372e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.424e-01, tolerance: 2.626e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.323e-02, tolerance: 2.481e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:40:07,668 [INFO] Best Params found: {'reg__alpha': 0.06578820119123178, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.3, 'reg__max_iter': 3000}
2025-05-22 14:40:07,668 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0279
2025-05-22 14:40:07,668 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:40:07,668 [INFO]   - Model type: en (Elastic Net)
2025-05-22 14:40:07,668 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:40:07,669 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', ElasticNet(alpha=0.06578820119123178, l1_ratio=0.3, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': ElasticNet(alpha=0.06578820119123178, l1_ratio=0.3, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.06578820119123178, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.3, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:40:07,669 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:40:07,669 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:40:07,669 [INFO]   - Alpha value: 0.06578820119123178
2025-05-22 14:40:07,701 [INFO] Hyperparameter Tuning Duration: 2.19 seconds
2025-05-22 14:40:07,701 [INFO] Predicting on outer test set...
2025-05-22 14:40:07,703 [INFO] Calculating performance metrics...
2025-05-22 14:40:07,737 [INFO] Generating diagnostic plots...
2025-05-22 14:40:08,427 [INFO] Diagnostic plots saved.
2025-05-22 14:40:08,427 [INFO] Prediction & Evaluation Duration: 0.73 seconds
2025-05-22 14:40:08,427 [INFO] Calculating and saving SHAP values...
2025-05-22 14:40:08,427 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:40:09,850 [INFO] SHAP Calculation & Saving Duration: 1.42 seconds
2025-05-22 14:40:09,851 [INFO] -- Outer Fold 4 finished. Duration: 4.34 seconds --
2025-05-22 14:40:09,851 [INFO] 
-- Processing Outer Fold 5/5 --
2025-05-22 14:40:09,853 [INFO] Train set size: (1348, 61), Test set size: (300, 61)
2025-05-22 14:40:09,853 [INFO] Test set indices range from 150 to 1548
2025-05-22 14:40:09,853 [INFO] Using GroupKFold for inner CV with 3 folds.
2025-05-22 14:40:09,853 [INFO] Starting hyperparameter tuning (RandomizedSearchCV)...
2025-05-22 14:40:09,854 [INFO] Linear model 'en' detected. Setting n_jobs=1 for efficient hyperparameter tuning.
2025-05-22 14:40:09,854 [INFO] Fitting with groups parameter for GroupKFold
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.224e+00, tolerance: 1.799e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.723e-01, tolerance: 2.740e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.448e+00, tolerance: 2.641e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.715e+00, tolerance: 1.799e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.161e+01, tolerance: 2.740e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.309e+01, tolerance: 2.641e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.344e-01, tolerance: 1.799e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.079e-01, tolerance: 2.641e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.644e-01, tolerance: 1.089e-02
  model = cd_fast.enet_coordinate_descent(
/home/cseomoon/miniconda3/envs/Abnb/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.962e-01, tolerance: 1.418e-02
  model = cd_fast.enet_coordinate_descent(
2025-05-22 14:40:11,890 [INFO] Best Params found: {'reg__alpha': 0.0745934328572655, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.5, 'reg__max_iter': 3000}
2025-05-22 14:40:11,890 [INFO] Best Inner CV Score (neg_mean_squared_error): -0.0368
2025-05-22 14:40:11,890 [INFO] 
[DEBUG] Linear model details:
2025-05-22 14:40:11,890 [INFO]   - Model type: en (Elastic Net)
2025-05-22 14:40:11,890 [INFO]   - Best estimator type: <class 'sklearn.pipeline.Pipeline'>
2025-05-22 14:40:11,890 [INFO]   - All params: {'memory': None, 'steps': [('scaler', StandardScaler()), ('reg', ElasticNet(alpha=0.0745934328572655, max_iter=3000))], 'transform_input': None, 'verbose': False, 'scaler': StandardScaler(), 'reg': ElasticNet(alpha=0.0745934328572655, max_iter=3000), 'scaler__copy': True, 'scaler__with_mean': True, 'scaler__with_std': True, 'reg__alpha': 0.0745934328572655, 'reg__copy_X': True, 'reg__fit_intercept': True, 'reg__l1_ratio': 0.5, 'reg__max_iter': 3000, 'reg__positive': False, 'reg__precompute': False, 'reg__random_state': None, 'reg__selection': 'cyclic', 'reg__tol': 0.0001, 'reg__warm_start': False}
2025-05-22 14:40:11,890 [INFO]   - Pipeline steps: ['scaler', 'reg']
2025-05-22 14:40:11,890 [INFO]   - Scaler found: <class 'sklearn.preprocessing._data.StandardScaler'>
2025-05-22 14:40:11,890 [INFO]   - Alpha value: 0.0745934328572655
2025-05-22 14:40:11,921 [INFO] Hyperparameter Tuning Duration: 2.07 seconds
2025-05-22 14:40:11,921 [INFO] Predicting on outer test set...
2025-05-22 14:40:11,924 [INFO] Calculating performance metrics...
2025-05-22 14:40:11,962 [INFO] Generating diagnostic plots...
2025-05-22 14:40:12,803 [INFO] Diagnostic plots saved.
2025-05-22 14:40:12,803 [INFO] Prediction & Evaluation Duration: 0.88 seconds
2025-05-22 14:40:12,803 [INFO] Calculating and saving SHAP values...
2025-05-22 14:40:12,803 [INFO] Sampling 100 instances from X_train for SHAP background data.
2025-05-22 14:40:13,996 [INFO] SHAP Calculation & Saving Duration: 1.19 seconds
2025-05-22 14:40:13,996 [INFO] -- Outer Fold 5 finished. Duration: 4.15 seconds --
2025-05-22 14:40:13,996 [INFO] 
--- Aggregating results for: Elastic Net (en) ---
2025-05-22 14:40:13,997 [INFO] Average Metrics across folds:
2025-05-22 14:40:13,997 [INFO] {
    "r2_mean": 0.7288513267944319,
    "r2_std": 0.17542816336742967,
    "mse_mean": 0.03401914132855805,
    "mse_std": 0.026374479509713524,
    "rmse_mean": 0.16801020972675512,
    "rmse_std": 0.07610329004799857,
    "mae_mean": 0.11936685352747482,
    "mae_std": 0.042959847383845784,
    "best_inner_cv_score_mean": -0.03485515356817762,
    "best_inner_cv_score_std": 0.008401266535150559,
    "scoring_metric_used": "neg_mean_squared_error"
}
2025-05-22 14:40:14,100 [INFO] Combined predictions saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/en/all_folds_predictions.csv
2025-05-22 14:40:14,100 [INFO] 
Running global SHAP analysis...
2025-05-22 14:40:15,715 [INFO] Global SHAP Analysis Duration: 1.61 seconds
2025-05-22 14:40:15,715 [INFO] Result Aggregation & Global SHAP Duration: 1.72 seconds
2025-05-22 14:40:15,715 [INFO] Average time per outer fold: 4.04 seconds
2025-05-22 14:40:15,715 [INFO] --- Nested CV completed for: Elastic Net (en) ---
2025-05-22 14:40:15,715 [INFO] Total execution time for model 'en': 21.93 seconds
2025-05-22 14:40:15,715 [INFO] 
--- Overall Training Summary ---
2025-05-22 14:40:15,740 [INFO] Model comparison summary saved to: /home/cseomoon/appl/af_analysis-0.1.4/model/regression/results/rg_rosetta_20250522_133452/model_comparison_summary.csv
2025-05-22 14:40:15,740 [INFO] Successful Model Summary:
2025-05-22 14:40:15,740 [INFO]   model_name   r2_mean  ...  best_inner_cv_score_std     scoring_metric_used
0         rf  0.705905  ...                 0.008785  neg_mean_squared_error
1       lgbm  0.653123  ...                 0.012569  neg_mean_squared_error
2        xgb  0.659062  ...                 0.009821  neg_mean_squared_error
3         lr  0.497788  ...                 0.030100  neg_mean_squared_error
4      ridge  0.728041  ...                 0.011260  neg_mean_squared_error
5      lasso  0.724196  ...                 0.008732  neg_mean_squared_error
6         en  0.728851  ...                 0.008401  neg_mean_squared_error

[7 rows x 12 columns]
2025-05-22 14:40:15,752 [INFO] Final Summary Saving Duration: 0.04 seconds
2025-05-22 14:40:15,752 [INFO] 
--- Framework Execution Finished ---
2025-05-22 14:40:15,752 [INFO] Total script execution time: 3916.06 seconds
