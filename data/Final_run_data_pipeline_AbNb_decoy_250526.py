#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import time
import logging
import pandas as pd
import subprocess
from pathlib import Path
from datetime import datetime
from contextlib import contextmanager
import af_analysis
from af_analysis import analysis
from af_analysis.data import Data

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'pipeline_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ë¡œê¹… ë ˆë²¨ ì„¤ì •
logging.getLogger('pdb_numpy').setLevel(logging.WARNING)
logging.getLogger('pdb_numpy.coor').setLevel(logging.WARNING)
logging.getLogger('pdb_numpy.analysis').setLevel(logging.WARNING)

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_PATH = '/home/cseomoon/project/ABAG/DB/AbNb_structure/AF3/decoy'
NATIVE_DIR = '/home/cseomoon/project/ABAG/DB/AbNb_structure/original_pdb'
OUTPUT_DIR = 'pipeline_results_AbNb_decoy'

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(OUTPUT_DIR, exist_ok=True)

def create_slurm_script(query_name, output_dir):
    """Slurm ì‘ì—… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    script_content = f"""#!/bin/bash
#SBATCH --job-name=af_{query_name}
#SBATCH --output={output_dir}/slurm_{query_name}_%j.log
#SBATCH --cpus-per-task=4
#SBATCH --partition=skylake_short,rome_short,milan_short
#SBATCH --exclude=gpu[1-10]

# í™˜ê²½ ì„¤ì •
source /home/cseomoon/miniconda3/etc/profile.d/conda.sh
conda activate Abnb

# Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python3 -c '
import sys
import os
import time
import pandas as pd
from contextlib import contextmanager
from af_analysis.data import Data
from af_analysis import analysis

@contextmanager
def time_tracker(description):
    start = time.time()
    print(f"ğŸ”„ Starting: {{description}}")
    yield
    elapsed = time.time() - start
    print(f"âœ… Completed: {{description}} ({{elapsed:.2f}}s)")

def calculate_all_metrics_per_query_optimized(query_group_data, **kwargs):
    query_name, query_df = query_group_data
    
    print(f"\\nğŸš€ Processing query: {{query_name}} ({{len(query_df)}} models)")
    total_start = time.time()
    
    try:
        # Data ê°ì²´ ìƒì„±
        with time_tracker("Data object creation"):
            data_dict = {{}}
            for col in query_df.columns:
                data_dict[col] = query_df[col].tolist()
            
            temp_data = Data(data_dict=data_dict)
        
        # 1. ê¸°ë³¸ AF3 metrics
        with time_tracker("Basic AF3 metrics"):
            temp_data = (temp_data
                        .extract_chain_columns(verbose=False)
                        .analyze_chains(verbose=False)
                        .add_h3_l3_plddt(verbose=False))
                
        # 2. Interface metrics
        with time_tracker("Interface metrics"):
            analysis.add_interface_metrics(temp_data, verbose=False)
        
        # 3. PPI metrics
        with time_tracker("pDockQ calculations"):
            analysis.pdockq(temp_data, verbose=False)
            analysis.pdockq2(temp_data, verbose=False)
        
        with time_tracker("LIS matrix"):
            analysis.LIS_matrix(temp_data, verbose=False)
        
        # 4. piTM/pIS
        with time_tracker("piTM/pIS calculation"):
            temp_data.add_pitm_pis(cutoff=8.0, verbose=False)
        
        # 5. RMSD metrics
        with time_tracker("RMSD calculations"):
            temp_data = (temp_data
                        .add_chain_rmsd(align_chain="A", rmsd_chain="H")
                        .add_rmsd_scale())
        
        # 6. Rosetta metrics
        with time_tracker("Rosetta interface metrics"):
            temp_data.add_rosetta_metrics(n_jobs=1, verbose=False)
        
        # # 7. DockQ
        # if kwargs.get("calculate_dockq", False):
        #     with time_tracker("DockQ calculation"):
        #         temp_data.prep_dockq(native_dir=kwargs.get("native_dir"), verbose=False)
        #         analysis.calculate_dockq(temp_data, 
        #                                rec_chains="A", lig_chains="H",
        #                                native_rec_chains="A", native_lig_chains="H",
        #                                verbose=False)
        
        total_time = time.time() - total_start
        print(f"âœ¨ Query {{query_name}} completed in {{total_time:.2f}}s")
        
        return query_name, temp_data.df
        
    except Exception as e:
        total_time = time.time() - total_start
        print(f"âŒ Error processing query {{query_name}} after {{total_time:.2f}}s: {{str(e)}}")
        import traceback
        print(traceback.format_exc())
        return query_name, None

# ì¿¼ë¦¬ ì²˜ë¦¬ í•¨ìˆ˜
def process_query(query_name):
    base_path = "{BASE_PATH}"
    target = os.path.join(base_path, query_name)
    
    try:
        # Data ê°ì²´ ìƒì„± ë° ì²˜ë¦¬
        my_data = Data(directory=target)
        df = my_data.df
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        for _, group_df in df.groupby("query"):
            name, result_df = calculate_all_metrics_per_query_optimized(
                (query_name, group_df),
                calculate_dockq=True,
                native_dir="{NATIVE_DIR}"
            )
            
            if result_df is not None:
                output_file = os.path.join("{output_dir}", f"metrics_{{name}}.csv")
                result_df.to_csv(output_file, index=False)
                print(f"Successfully processed {{query_name}}")
            else:
                print(f"Failed to process {{query_name}}")
                
    except Exception as e:
        print(f"Error processing {{query_name}}: {{str(e)}}")
        return False
    
    return True

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    query_name = "{query_name}"
    process_query(query_name)
'
"""
    script_path = os.path.join(output_dir, f"run_{query_name}.sh")
    with open(script_path, 'w') as f:
        f.write(script_content)
    
    return script_path

def submit_jobs(queries, max_concurrent=50):
    """Slurm ì‘ì—… ì œì¶œ ë° ëª¨ë‹ˆí„°ë§"""
    def get_available_cpu_nodes():
        """ì‚¬ìš© ê°€ëŠ¥í•œ CPU ë…¸ë“œ ìˆ˜ í™•ì¸"""
        result = subprocess.run(['sinfo', '-p', 'skylake_normal,rome_normal', '-h', '-o', '%C'],
                              capture_output=True, text=True)
        if result.returncode == 0:
            # ì‚¬ìš© ê°€ëŠ¥í•œ CPU ìˆ˜ íŒŒì‹±
            available = result.stdout.strip()
            return int(available.split('/')[1])  # ì „ì²´ CPU ìˆ˜
        return 0

    running_jobs = set()
    completed_jobs = set()
    failed_jobs = set()
    
    for query in queries:
        # ì‚¬ìš© ê°€ëŠ¥í•œ CPU ë…¸ë“œ í™•ì¸
        available_cpus = get_available_cpu_nodes()
        if available_cpus < 4:  # ê° ì‘ì—…ì´ 4ê°œì˜ CPUë¥¼ ì‚¬ìš©
            logger.warning(f"Not enough CPU resources available. Waiting...")
            time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
            continue

        # ì‹¤í–‰ ì¤‘ì¸ ì‘ì—…ì´ ìµœëŒ€ ê°œìˆ˜ì— ë„ë‹¬í•˜ë©´ ëŒ€ê¸°
        while len(running_jobs) >= max_concurrent:
            # ì‘ì—… ìƒíƒœ í™•ì¸
            for job_id in list(running_jobs):
                result = subprocess.run(['squeue', '-j', str(job_id)], 
                                     capture_output=True, text=True)
                if str(job_id) not in result.stdout:
                    running_jobs.remove(job_id)
                    # ê²°ê³¼ íŒŒì¼ í™•ì¸
                    output_file = os.path.join(OUTPUT_DIR, f"metrics_{query}.csv")
                    if os.path.exists(output_file):
                        completed_jobs.add(query)
                    else:
                        failed_jobs.add(query)
            time.sleep(10)
        
        # ìƒˆ ì‘ì—… ì œì¶œ
        script_path = create_slurm_script(query, OUTPUT_DIR)
        try:
            # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
            os.chmod(script_path, 0o755)
            
            # ì‘ì—… ì œì¶œ
            result = subprocess.run(['sbatch', script_path], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                job_id = result.stdout.split()[-1]
                running_jobs.add(job_id)
                logger.info(f"Submitted job {job_id} for query {query}")
            else:
                logger.error(f"Failed to submit job for query {query}")
                logger.error(f"Error output: {result.stderr}")
                logger.error(f"Command output: {result.stdout}")
                failed_jobs.add(query)
                
        except Exception as e:
            logger.error(f"Exception while submitting job for query {query}: {str(e)}")
            failed_jobs.add(query)
    
    return completed_jobs, failed_jobs

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì¿¼ë¦¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    queries = [d for d in os.listdir(BASE_PATH) 
              if os.path.isdir(os.path.join(BASE_PATH, d))]
    
    logger.info(f"Found {len(queries)} queries to process")
    
    # ì‘ì—… ì œì¶œ ë° ëª¨ë‹ˆí„°ë§
    completed, failed = submit_jobs(queries)
    
    # ê²°ê³¼ ìš”ì•½
    logger.info(f"Processing completed:")
    logger.info(f"Successfully processed: {len(completed)} queries")
    logger.info(f"Failed to process: {len(failed)} queries")
    
    if failed:
        logger.info("Failed queries:")
        for query in failed:
            logger.info(f"- {query}")

if __name__ == "__main__":
    main()
